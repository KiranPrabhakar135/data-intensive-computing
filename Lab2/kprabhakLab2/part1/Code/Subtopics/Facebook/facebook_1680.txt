ai could worsen health disparitiesin health system riddled inequity risk making dangerous biases automated invisibledr khullar assistant professor health care policy researchartificial intelligence beginning meet and sometimes exceed assessments doctors various clinical situations ai diagnose skin cancer like dermatologists seizures like neurologists diabetic retinopathy like ophthalmologists algorithms developed predict patients get diarrhea end icu fda recently approved first machine learning algorithm measure much blood flows heart — tedious timeconsuming calculation traditionally done cardiologists it’s enough make doctors like wonder spent decade medical training learning art diagnosis treatment many questions whether ai actually works medicine works pick pneumonia detect cancer predict death questions focus technical ethical health system riddled inequity ask could use ai medicine worsen health disparities least three reasons believe mightthe first training problem ai must learn diagnose disease large data sets data doesn’t include enough patients particular background won’t reliable them evidence fields suggests isn’t theoretical concern recent study found facial recognition programs incorrectly classify less percent lightskinned men onethird darkskinned women happens rely algorithms diagnose melanoma light versus dark skinmedicine long struggled include enough women minorities research despite knowing different risk factors manifestations disease many genetic studies suffer dearth black patients leading erroneous conclusions women often experience different symptoms heart attack causing delays treatment perhaps widely used cardiovascular risk score developed using data mostly white patients less precise minoritieswill using ai tell us might stroke patients benefit clinical trial codify concerns algorithms prove less effective underrepresented groupssecond ai trained realworld data risks incorporating entrenching perpetuating economic social biases contribute health disparities first place again evidence fields instructive ai programs used help judges predict criminals likely reoffend shown troubling racial biases designed help child protective services decide calls require investigationin medicine unchecked ai could create selffulfilling prophesies confirm preexisting biases especially used conditions complex tradeoffs high degrees uncertainty if example poorer patients worse organ transplantation receiving chemotherapy endstage cancer machine learning algorithms may conclude patients less likely benefit treatment — recommend itfinally even ostensibly fair neutral ai potential worsen disparities implementation disproportionate effects certain groups consider program helps doctors decide whether patient go home rehab facility knee surgery it’s decision imbued uncertainty real consequences evidence suggests discharge institution associated higher costs higher risk readmission algorithm incorporates residence lowincome neighborhood marker poor social support may recommend minority patients go nursing facilities instead receive homebased physical therapy worse yet program designed maximize efficiency lower medical costs might discourage operating patients altogetherto extent problems already exist medicine american health care always struggled income racebased inequities rooted various forms bias risk ai biases become automated invisible — begin accept wisdom machines wisdom clinical moral intuition many ai programs black boxes don’t know exactly what’s going inside produce output do may increasingly expected honor recommendations practice i’ve often seen tool quickly become crutch — excuse outsource decision making someone something else medical students struggling interpret ekg inevitably peek computergenerated output top sheet often swayed report provided alongside chest xray ct scan automation becomes pervasive catch spellcheck autocorrected “they’re” “there” meant “their”still ai holds tremendous potential improve medicine may well make care efficient accurate — properly deployed — equitable realizing promise requires aware potential bias guarding it means regularly monitoring output algorithms downstream consequences cases necessitate counterbias algorithms hunt correct subtle systematic discriminationbut fundamentally means recognizing humans machines still responsible caring patients duty ensure we’re using ai another tool disposal — way arounddhruv khullar dhruvkhullar doctor newyorkpresbyterian hospital assistant professor departments medicine health care policy weill cornell medicine director policy dissemination physicians foundation center study physician practice leadershipfollow new york times opinion section facebook twitter nytopinion instagram